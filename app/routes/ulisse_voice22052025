from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel
from sqlalchemy.orm import Session
from sqlalchemy import text
from openai import OpenAI
import os
import json
from app.core.database import get_db
from matplotlib import pyplot as plt
import io
import base64
import pandas as pd

router = APIRouter(prefix="/ulisse/voice", tags=["ulisse"])

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
MODEL = os.getenv("OPENAI_MODEL", "gpt-4-turbo")

PRIORITY_MAP = {0: "bassa", 1: "media", 2: "alta"}
STATUS_MAP = {0: "aperto", 1: "sospeso", 2: "chiuso"}

class TextInput(BaseModel):
    text: str

@router.post("/nlm_query")
def query_from_text(payload: TextInput, db: Session = Depends(get_db)):
    try:
        prompt = (
            "Agisci come analista dati per un CRM. Genera una query SQL per PostgreSQL compatibile con il nostro schema.\n"
            "Restituisci un JSON del tipo: {sql: '...', answer_mode: 'discorsiva|tabellare|grafico|predittiva'}.\n"
            "Se la risposta Ã¨ un singolo valore o una riga, imposta answer_mode=discorsiva.\n"
            "Schema: activities(id, title, priority INT), tickets(id, status INT, priority INT), tasks(id, ticket_id).\n"
            "Usa solo valori numerici nei filtri, es. status = 0 per \"aperto\". Evita stringhe come \"open\".\n"
            f"Domanda: {payload.text}"
        )

        response = client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "system", "content": "Sei un assistente SQL esperto per CRM."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2
        )

        reply = response.choices[0].message.content.strip()

        # ðŸ§¹ Rimozione blocco markdown
        if reply.startswith("```json"):
            reply = reply.strip("`").strip("json").strip()

        try:
            parsed = json.loads(reply)
        except json.JSONDecodeError:
            import ast
            try:
                parsed = ast.literal_eval(reply)
                if isinstance(parsed, str):
                    parsed = json.loads(parsed)
            except Exception:
                raise HTTPException(status_code=422, detail=f"Risposta GPT non in formato JSON valido: {reply}")

        sql = parsed.get("sql")
        mode = parsed.get("answer_mode")

        if not sql or not mode:
            raise ValueError("Risposta GPT incompleta")

        result = db.execute(text(sql))
        rows = result.fetchall()
        columns = result.keys()
        df = pd.DataFrame(rows, columns=columns)

        print("âœ… Risultato tabella:", df.to_dict(orient="records"))

        # Mappatura prioritÃ /status
        for col in df.columns:
            if "priority" in col:
                df[col] = df[col].map(PRIORITY_MAP).fillna(df[col])
            if "status" in col:
                df[col] = df[col].map(STATUS_MAP).fillna(df[col])

        # Tabellare
        if mode == "tabellare":
            if df.shape == (1, 1):
                mode = "discorsiva"
            else:
                return df.to_dict(orient="records")

        # Grafico
        if mode == "grafico":
            img = io.BytesIO()
            df.select_dtypes(include=['object', 'category']).apply(lambda x: x.value_counts()).T.plot(kind='bar')
            plt.tight_layout()
            plt.savefig(img, format='png')
            plt.close()
            img.seek(0)
            b64 = base64.b64encode(img.read()).decode("utf-8")
            return {"image_base64": b64}

        # Discorsiva (risposta naturale)
        if mode == "discorsiva":
            if df.shape == (1, 1):
                col = df.columns[0]
                val = df.iat[0, 0]
                desc_prompt = (
                    f"L'utente ha chiesto: \"{payload.text}\"\n"
                    f"Il valore di risposta Ã¨: {col} = {val}\n"
                    f"Restituisci una risposta breve in italiano, naturale, discorsiva, senza codice nÃ© spiegazioni tecniche."
                )
                follow_up = client.chat.completions.create(
                    model=MODEL,
                    messages=[{"role": "user", "content": desc_prompt}]
                )
                return {"summary": follow_up.choices[0].message.content.strip()}

            desc_prompt = f"Descrivi in italiano il significato della seguente tabella:\n{df.to_string()}"
            follow_up = client.chat.completions.create(
                model=MODEL,
                messages=[{"role": "user", "content": desc_prompt}]
            )
            return {"summary": follow_up.choices[0].message.content.strip()}

        # Predittiva
        if mode == "predittiva":
            return {"warning": "Predizione non ancora implementata"}

        return {"summary": "Nessun dato utile trovato."}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
